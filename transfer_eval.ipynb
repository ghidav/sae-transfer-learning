{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from sae_lens import SAE, ActivationsStore, HookedSAETransformer\n",
    "from sae_lens.config import LanguageModelSAERunnerConfig\n",
    "from sae_lens.evals import EvalConfig, run_evals\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\"mech-interp/pythia-160m-deduped-rs-post\", local_dir=\"./checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"pythia-160m-deduped\",\n",
    "    hook_name=None,\n",
    "    hook_layer=None,\n",
    "    dataset_path=\"NeelNanda/pile-small-tokenized-2b\",\n",
    "    is_dataset_tokenized=True,\n",
    "    context_size=1024,\n",
    "    streaming=True,\n",
    "    # SAE Parameters\n",
    "    architecture=\"jumprelu\",\n",
    "    d_in=768,\n",
    "    d_sae=None,\n",
    "    b_dec_init_method=\"zeros\",\n",
    "    expansion_factor=8,\n",
    "    activation_fn=\"relu\",  # relu, tanh-relu, topk\n",
    "    normalize_sae_decoder=True,\n",
    "    from_pretrained_path=None,\n",
    "    apply_b_dec_to_input=False,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    dtype=\"float32\",\n",
    "    prepend_bos=False,\n",
    ")\n",
    "\n",
    "eval_cfg = EvalConfig(\n",
    "    batch_size_prompts=8,\n",
    "    # Reconstruction metrics\n",
    "    n_eval_reconstruction_batches=32,\n",
    "    compute_kl=True,\n",
    "    compute_ce_loss=True,\n",
    "    # Sparsity and variance metrics\n",
    "    n_eval_sparsity_variance_batches=1,\n",
    "    compute_l2_norms=True,\n",
    "    compute_sparsity_metrics=True,\n",
    "    compute_variance_metrics=False,\n",
    ")\n",
    "\n",
    "\n",
    "def update_cfg(act_layer, hook_name):\n",
    "    default_cfg.hook_layer = act_layer\n",
    "    default_cfg.hook_name = f\"blocks.{act_layer}.{hook_name}\"\n",
    "    return default_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = \"RES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if component == \"RES\":\n",
    "    component = \"rs-post\"\n",
    "    hook_name = \"hook_resid_post\"\n",
    "elif component == \"MLP\":\n",
    "    component = \"mlp-out\"\n",
    "    hook_name = \"hook_mlp_out\"\n",
    "elif component == \"ATT\":\n",
    "    component = \"attn-z\"\n",
    "    hook_name = \"attn.hook_z\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid component.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = HookedSAETransformer.from_pretrained(\"pythia-160m-deduped\").to(device)\n",
    "\n",
    "layers = model.cfg.n_layers\n",
    "all_metrics = []\n",
    "\n",
    "for i in tqdm(range(layers)):\n",
    "    # Set activation store\n",
    "    cfg = update_cfg(i, hook_name)\n",
    "    activations_store = ActivationsStore.from_config(model, cfg)\n",
    "    for j in range(layers):\n",
    "        try:\n",
    "            # Load SAE\n",
    "            SAE_PATH = f\"checkpoints/L{j}/\"\n",
    "            sae = SAE.load_from_pretrained(SAE_PATH).to(device)\n",
    "\n",
    "            metrics = run_evals(sae, activations_store, model, eval_cfg)\n",
    "            metrics = {k.split(\"/\")[-1]: v for k, v in metrics.items()}\n",
    "            print(f\"L{j} SAE on L{i} activations. C/E: {metrics['ce_loss_score']:.3f}\")\n",
    "            all_metrics.append(pd.Series(metrics, name=f\"{i}-{j}\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load L{j} SAE.\", e)\n",
    "            continue\n",
    "\n",
    "all_metrics = pd.concat(all_metrics, axis=1).T\n",
    "all_metrics.to_csv(f\"eval/{component}_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer eval - Backward (i -> i-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = \"backward\"\n",
    "ckpt_folder = f\"/root/sae-transfer-learning/checkpoints/{direction}_TL\"\n",
    "ckpt_step = \"final_500002816\"\n",
    "mapping = {}\n",
    "for _dir in os.listdir(ckpt_folder):\n",
    "    try:\n",
    "        cfg = json.load(open(os.path.join(ckpt_folder, _dir, ckpt_step, \"cfg.json\")))\n",
    "        mapping[_dir] = f\"L{cfg['hook_name'].split('.')[1]}\"\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "inv_mapping = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = HookedSAETransformer.from_pretrained(\"pythia-160m-deduped\").to(device)\n",
    "checkpoints = [\"100003840\", \"200003584\", \"300003328\", \"400003072\", \"final_500002816\"]\n",
    "\n",
    "\n",
    "start_layer = 1\n",
    "end_layer = model.cfg.n_layers\n",
    "\n",
    "direction = \"backward\"\n",
    "ckpt_folder = f\"/root/sae-transfer-learning/checkpoints/{direction}_TL\"\n",
    "\n",
    "for ckpt_step in checkpoints:\n",
    "    all_transfer_metrics = []\n",
    "    for sae_idx in tqdm(range(start_layer, end_layer)):\n",
    "        # Set activation store\n",
    "        for act_offset in range(-1, 1):\n",
    "            act_idx = sae_idx + act_offset\n",
    "            cfg = update_cfg(act_idx, hook_name)\n",
    "            activations_store = ActivationsStore.from_config(model, cfg)\n",
    "            try:\n",
    "                # Load SAE\n",
    "                TRANSFER_SAE_PATH = os.path.join(ckpt_folder, inv_mapping[f\"L{sae_idx-1}\"], ckpt_step)\n",
    "                sae = SAE.load_from_pretrained(TRANSFER_SAE_PATH).to(device)\n",
    "\n",
    "                metrics = run_evals(sae, activations_store, model, eval_cfg)\n",
    "                metrics = {k.split(\"/\")[-1]: v for k, v in metrics.items()}\n",
    "                print(f\"L{sae_idx} SAE on L{act_idx} activations. C/E: {metrics['ce_loss_score']:.3f}\")\n",
    "                all_transfer_metrics.append(pd.Series(metrics, name=f\"{act_idx}-{sae_idx}\"))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load L{sae_idx} SAE.\", e)\n",
    "                continue\n",
    "\n",
    "    all_transfer_metrics = pd.concat(all_transfer_metrics, axis=1).T\n",
    "    all_transfer_metrics.to_csv(f\"eval/{component}_transfer_backward_{ckpt_step}_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer eval - forward (i-1 -> i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = \"forward\"\n",
    "ckpt_folder = f\"/root/sae-transfer-learning/checkpoints/{direction}_TL\"\n",
    "ckpt_step = \"final_500002816\"\n",
    "mapping = {}\n",
    "for _dir in os.listdir(ckpt_folder):\n",
    "    try:\n",
    "        cfg = json.load(open(os.path.join(ckpt_folder, _dir, ckpt_step, \"cfg.json\")))\n",
    "        mapping[_dir] = f\"L{cfg['hook_name'].split('.')[1]}\"\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "inv_mapping = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = HookedSAETransformer.from_pretrained(\"pythia-160m-deduped\").to(device)\n",
    "checkpoints = [\"100003840\", \"200003584\", \"300003328\", \"400003072\", \"final_500002816\"]\n",
    "\n",
    "start_layer = 0\n",
    "end_layer = model.cfg.n_layers - 1\n",
    "\n",
    "direction = \"forward\"\n",
    "ckpt_folder = f\"/root/sae-transfer-learning/checkpoints/{direction}_TL\"\n",
    "\n",
    "for ckpt_step in checkpoints:\n",
    "    print(f\"Checkpoint: {ckpt_step}\")\n",
    "    all_transfer_metrics = []\n",
    "    for sae_idx in tqdm(range(start_layer, end_layer)):\n",
    "        # Set activation store\n",
    "        for act_offset in range(0, 2):\n",
    "            act_idx = sae_idx + act_offset\n",
    "            cfg = update_cfg(act_idx, hook_name)\n",
    "            activations_store = ActivationsStore.from_config(model, cfg)\n",
    "            try:\n",
    "                # Load SAE\n",
    "                TRANSFER_SAE_PATH = os.path.join(ckpt_folder, inv_mapping[f\"L{sae_idx+1}\"], ckpt_step)\n",
    "                sae = SAE.load_from_pretrained(TRANSFER_SAE_PATH).to(device)\n",
    "\n",
    "                metrics = run_evals(sae, activations_store, model, eval_cfg)\n",
    "                metrics = {k.split(\"/\")[-1]: v for k, v in metrics.items()}\n",
    "                print(f\"L{sae_idx} SAE on L{act_idx} activations. C/E: {metrics['ce_loss_score']:.3f}\")\n",
    "                all_transfer_metrics.append(pd.Series(metrics, name=f\"{act_idx}-{sae_idx}\"))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load L{sae_idx} SAE.\", e)\n",
    "                continue\n",
    "\n",
    "    all_transfer_metrics = pd.concat(all_transfer_metrics, axis=1).T\n",
    "    all_transfer_metrics.to_csv(f\"eval/{component}_transfer_forward_{ckpt_step}_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
