{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6cfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from tqdm.notebook import tqdm\n",
    "import transformer_lens\n",
    "from transformer_lens.HookedTransformer import HookedTransformer, HookedTransformerConfig\n",
    "from datasets import Dataset\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, Trainer, TrainingArguments, PushToHubCallback, DataCollatorForLanguageModeling\n",
    "from huggingface_hub import HfFolder\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('sae-transfer-learning')\n",
    "from convert_model import convert_model\n",
    "\n",
    "# Log in to Hugging Face\n",
    "HfFolder.save_token('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')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme('notebook', 'darkgrid')\n",
    "palette = sns.color_palette('colorblind')\n",
    "\n",
    "DEVICE = transformer_lens.utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed047bf-8dc3-4eb2-bd25-be1375781c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9127d9219a894246a733fbe5ceba667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74334e7ecf914604958c2f0c2824f21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bd0b1733284a92a325ae57fe2a076d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367c4b924e914d4c80dbd2a4833e1474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183ae67c1a6f4ecf8fea8a4742165fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = GPTNeoXForCausalLM.from_pretrained(\"marco-molinari/pythia-70m-instruct\")\n",
    "base_model = transformer_lens.HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "pythia_cfg = base_model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4dc0533-d99c-41a7-8f22-813a6ed116ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_tl_model = convert_model(fine_tuned_model, pythia_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e5e741-9f58-4571-8505-e0adf85af1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Decoded Output: <|endoftext|>User: Name 3 vegetables. Assistant: \n",
      "1. Mushrooms: Mushrooms are a major component in the defense of the body. \n",
      "2. Mashed Potatoes: Mashed Potatoes are a major component in the defense of the body. \n",
      "3. Mushrooms: Mushrooms are a major component in the defense of the body. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"User: Name 3 vegetables. Assistant: \"\n",
    "tokens = fine_tuned_tl_model.to_tokens(reference_text)\n",
    "\n",
    "logits, cache = fine_tuned_tl_model.run_with_cache(tokens)\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "# Initialize next_tokens with the original tokens\n",
    "next_tokens = tokens\n",
    "\n",
    "# Generate the next 10 tokens\n",
    "for _ in range(65):\n",
    "    # Get the logits for the current sequence\n",
    "    logits = fine_tuned_tl_model(next_tokens)\n",
    "    # Get the next token by taking the argmax of the logits\n",
    "    next_token = logits[0, -1].argmax(dim=-1)\n",
    "    # Append the new token to the sequence\n",
    "    next_tokens = torch.cat([next_tokens, next_token[None, None]], dim=-1)\n",
    "\n",
    "\n",
    "print(\"Final Decoded Output:\", fine_tuned_tl_model.tokenizer.decode(next_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a8d67-d2d3-411a-b4d5-8b1054364ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca8b0aeffb2b7a98f29ba379058093df1fc88a600669642dc79207c759528fb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
