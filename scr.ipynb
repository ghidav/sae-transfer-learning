{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b)0\u001b7\u001b[?47h\u001b[1;24r\u001b[m\u001b[4l\u001b[?1h\u001b=Sun Aug 04 14:46:59 2024\n",
      "╒═════════════════════════════════════════════════════════════════════════════╕\n",
      "│ NVITOP 1.3.2        Driver Version: 550.100       CUDA Driver Version: 12.4 │\n",
      "├───────────────────────────────┬──────────────────────┬──────────────────────┤\n",
      "│ GPU  Name        Persistence-M│ Bus-Id        Disp.A │ Volatile Uncorr. ECC │\n",
      "│ Fan  Temp  Perf  Pwr:Usage/Cap│         Memory-Usage │ GPU-Util  Compute M. │\n",
      "╞═══════════════════════════════╪══════════════════════╪══════════════════════╡\n",
      "│\u001b[33m   0  GeForce RTX 3090    On   \u001b[0m│\u001b[33m 00000000:C3:00.0 Off \u001b[0m│\u001b[33m                  N/A \u001b[0m│\n",
      "│\u001b[33m 86%   65C    P8    36W / 350W \u001b[0m│\u001b[33m   9642MiB / 24.00GiB \u001b[0m│\u001b[33m      0%      Default \u001b[0m│\n",
      "├───────────────────────────────┼──────────────────────┼──────────────────────┤\n",
      "│\u001b[33m   1  GeForce RTX 3090    On   \u001b[0m│\u001b[33m 00000000:E1:00.0 Off \u001b[0m│\u001b[33m                  N/A \u001b[0m│\n",
      "│\u001b[33m 44%   59C    P8    18W / 350W \u001b[0m│\u001b[33m   9642MiB / 24.00GiB \u001b[0m│\u001b[33m      0%      Default \u001b[0m│\n",
      "╘═══════════════════════════════╧══════════════════════╧══════════════════════╛\n",
      "\u001b[1m\u001b[36m[ CPU: ▎ 0.9%            UPTIME: 7.4 days ]\u001b[0m  \u001b[1m( Load Average: 75.55 31.87 18.39 )\u001b[0m\n",
      "\u001b[1m\u001b[35m[ MEM: ███████▉ 26.3%      USED: 127.8GiB ]\u001b[0m  \u001b[1m\u001b[34m[ SWP: ██████████████████▍ 83.5%  ]\u001b[0m\n",
      "\n",
      "╒══════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Processes:                                                 \u001b[1m\u001b[33mroot\u001b[0m\u001b[1m@\u001b[0m\u001b[1m\u001b[32m66ab0999f2d8\u001b[0m │\n",
      "│ GPU     PID      USER  GPU-MEM %SM  %CPU  %MEM  TIME  COMMAND                │\n",
      "╞══════════════════════════════════════════════════════════════════════════════╡\n",
      "│\u001b[33m   0\u001b[0m  593748 C     N/A  9636MiB   0   N/A   N/A   N/A  \u001b[31mNo Such Process\u001b[0m        │\n",
      "├──────────────────────────────────────────────────────────────────────────────┤\n",
      "│\u001b[33m   1\u001b[0m  594237 C     N/A  9636MiB   0   N/A   N/A   N/A  \u001b[31mNo Such Process\u001b[0m        │\n",
      "╘══════════════════════════════════════════════════════════════════════════════╛\n",
      "\u001b[1m\u001b[31mERROR:\u001b[0m Failed to initialize `curses` (curs_set() returned ERR)\n"
     ]
    }
   ],
   "source": [
    "!nvitop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.10/site-packages/transformer_lens/utils.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model NeelNanda/GELU_1L512W_C4_Code into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"NeelNanda/GELU_1L512W_C4_Code\", device = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg.d_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit import IOICircuit\n",
    "import json\n",
    "\n",
    "with open('tasks/ioi/cfg.json') as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "with open('tasks/ioi/names.json') as f:\n",
    "    names = json.load(f)\n",
    "\n",
    "with open('tasks/ioi/prompts.json') as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "circuit = IOICircuit(\n",
    "    model= model,\n",
    "    cfg=cfg, \n",
    "    names=names, \n",
    "    prompts=prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Then, Bella and John had lots of fun at the clinic. John gave a pillow to',\n",
       " {'IO': ' Bella',\n",
       "  'POS': 'ABB',\n",
       "  'END': 18,\n",
       "  'IO_pos': 3,\n",
       "  'S1_pos': 5,\n",
       "  'S2_pos': 14,\n",
       "  'S': ' John'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = circuit.prompts[0]\n",
    "p.text, p.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then, John and Bella had lots of fun at the clinic. John gave a pillow to'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.generate_corrupted('POS', model).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>IO</th>\n",
       "      <th>S</th>\n",
       "      <th>POS</th>\n",
       "      <th>IO_pos</th>\n",
       "      <th>S1_pos</th>\n",
       "      <th>S1+1_pos</th>\n",
       "      <th>S2_pos</th>\n",
       "      <th>END</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Then, Bella and John had lots of fun at the cl...</td>\n",
       "      <td>Bella</td>\n",
       "      <td>John</td>\n",
       "      <td>ABB</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Then, Ross and Adrian were working at the jers...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>Adrian</td>\n",
       "      <td>ABB</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Then, Paul and Carl had lots of fun at the man...</td>\n",
       "      <td>Paul</td>\n",
       "      <td>Carl</td>\n",
       "      <td>ABB</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Then, Olivia and Ray had lots of fun at the ra...</td>\n",
       "      <td>Olivia</td>\n",
       "      <td>Ray</td>\n",
       "      <td>ABB</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Then, Christopher and Noah were working at the...</td>\n",
       "      <td>Christopher</td>\n",
       "      <td>Noah</td>\n",
       "      <td>ABB</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt            IO        S  \\\n",
       "0  Then, Bella and John had lots of fun at the cl...         Bella     John   \n",
       "1  Then, Ross and Adrian were working at the jers...          Ross   Adrian   \n",
       "2  Then, Paul and Carl had lots of fun at the man...          Paul     Carl   \n",
       "3  Then, Olivia and Ray had lots of fun at the ra...        Olivia      Ray   \n",
       "4  Then, Christopher and Noah were working at the...   Christopher     Noah   \n",
       "\n",
       "   POS  IO_pos  S1_pos  S1+1_pos  S2_pos  END  \n",
       "0  ABB       3       5         6      14   18  \n",
       "1  ABB       3       5         6      12   18  \n",
       "2  ABB       3       5         6      14   18  \n",
       "3  ABB       3       5         6      14   18  \n",
       "4  ABB       3       5         6      12   18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit.create_task_df()\n",
    "circuit.task_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:15<00:00, 10.08it/s]\n"
     ]
    }
   ],
   "source": [
    "circuit.get_activations(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:19<00:00, 11.07it/s]\n"
     ]
    }
   ],
   "source": [
    "circuit.compute_supervised_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit.IO_features[' Adam']['k']['IO'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 16.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "bs = 64\n",
    "\n",
    "activations = {i: [] for i in ['q', 'k', 'v', 'z']}\n",
    "for b in tqdm(range(0, len(task_df), bs)):\n",
    "    tokens = model.to_tokens(task_df.iloc[b:b+bs, 0])\n",
    "\n",
    "    positions = task_df.iloc[b:b+bs][['IO_pos', 'S1_pos', 'S1+1_pos', 'S2_pos', 'END']].values\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "\n",
    "    for key in activations.keys():\n",
    "        key_acts = cache.stack_activation(key) # [l b pos h dm]\n",
    "        activations[key].append(torch.cat([key_acts[:, None, i, pos] for i, pos in enumerate(positions)], dim=1))\n",
    "\n",
    "    del cache\n",
    "\n",
    "activations = {key: torch.cat(values, 1).cpu() for key, values in activations.items()} # [l N 5 h dm]\n",
    "mean_activations = {key: torch.mean(values, 1).cpu() for key, values in activations.items()} # [l 5 h dm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_dictionary(df, activations):\n",
    "    \n",
    "    io_vec = {}\n",
    "    s_vec = {}\n",
    "\n",
    "    attributes = ['IO', 'S1', 'S1+1', 'S2', 'END']\n",
    "\n",
    "    unique_io = df['IO'].unique()\n",
    "    unique_s1 = df['S1'].unique()\n",
    "\n",
    "    centered_activations = {c: activations[c] - activations[c].mean(1)[:, None] for c in ['q', 'k', 'v', 'z']}\n",
    "\n",
    "    for name in tqdm(set(unique_io) | set(unique_s1)):\n",
    "        io_vec[name] = {}\n",
    "        s_vec[name] = {}\n",
    "\n",
    "        for c in ['q', 'k', 'v', 'z']:\n",
    "            mask = torch.tensor(df['IO'] == name, dtype=torch.bool, device=activations[c].device)\n",
    "            if mask.any():\n",
    "                filtered_activations = centered_activations[c][:, mask] # [l M 5 h dm]\n",
    "                io_vec[name][c] = {a: filtered_activations[:, :, i].mean(1) for i, a in enumerate(attributes)}\n",
    "\n",
    "            mask = torch.tensor(df['S1'] == name, dtype=torch.bool, device=activations[c].device)\n",
    "            if mask.any():\n",
    "                filtered_activations = centered_activations[c][:, mask]\n",
    "                s_vec[name][c] = {a: filtered_activations[:, :, i].mean(1) for i, a in enumerate(attributes)}\n",
    "\n",
    "    # Pos\n",
    "    pos_vec = {'ABB': {}, 'BAB': {}}\n",
    "    \n",
    "    for c in ['q', 'k', 'v', 'z']:\n",
    "        for pos in [0, 1]:\n",
    "            mask = torch.tensor(df['POS'] == pos, dtype=torch.bool, device=activations[c].device)\n",
    "            if mask.any():\n",
    "                filtered_activations = centered_activations[c][:, mask]\n",
    "                pos_vec['ABB' if pos == 0 else 'BAB'][c] = {a: filtered_activations[:, :, i].mean(1) for i, a in enumerate(attributes)}\n",
    "    \n",
    "    return (io_vec, s_vec, pos_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:19<00:00, 11.27it/s]\n"
     ]
    }
   ],
   "source": [
    "f = supervised_dictionary(task_df, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0][' Richard']['q']['IO'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IOIPrompt.__init__() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m circuit\u001b[38;5;241m.\u001b[39mprompts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext, \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_corrupted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/workspace/sae-transfer-learning/circuit.py:61\u001b[0m, in \u001b[0;36mIOIPrompt.generate_corrupted\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     59\u001b[0m corr_prompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIO_pos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m s1_pos\n\u001b[1;32m     60\u001b[0m corr_prompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS1_pos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m io_pos\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mIOIPrompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorr_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: IOIPrompt.__init__() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "circuit.prompts[0].text, circuit.prompts[0].generate_corrupted('POS').get_variable('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Then, Bella and John had lots of fun at the clinic. John gave a pillow to', 'variables': {'IO': ' Bella', 'POS': 'ABB', 'END': 18, 'IO_pos': 3, 'S1_pos': 5, 'S2_pos': 14, 'S': ' John'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'S1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[0;32m----> 3\u001b[0m     p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS1+1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvariables\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'S1'"
     ]
    }
   ],
   "source": [
    "for p in prompts:\n",
    "    print(p)\n",
    "    p['variables']['S1+1'] = p['variables']['S1_pos'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 28 16:24:21 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:00:07.0 Off |                  Off |\n",
      "| 30%   50C    P2              87W / 300W |  26212MiB / 49140MiB |     15%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
