{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, ActivationsStore, LanguageModelSAERunnerConfig\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "palette = sns.color_palette('flare', as_cmap=False, n_colors=10)\n",
    "palette_duo = [palette[0], palette[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"pythia-160m-deduped\",\n",
    "    hook_name=None,\n",
    "    hook_layer=None,\n",
    "    dataset_path=\"NeelNanda/pile-small-tokenized-2b\",\n",
    "    is_dataset_tokenized=True,\n",
    "    context_size=1024,\n",
    "    streaming=True,\n",
    "    # SAE Parameters\n",
    "    architecture=\"jumprelu\",\n",
    "    d_in=768,\n",
    "    d_sae=None,\n",
    "    b_dec_init_method=\"zeros\",\n",
    "    expansion_factor=8,\n",
    "    activation_fn=\"relu\",  # relu, tanh-relu, topk\n",
    "    normalize_sae_decoder=True,\n",
    "    from_pretrained_path=None,\n",
    "    apply_b_dec_to_input=False,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    dtype=\"float32\",\n",
    "    prepend_bos=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-160m-deduped\", device = device)\n",
    "\n",
    "direction = \"backward\"\n",
    "ckpt_step = \"500M\"\n",
    "sae_idx = 8\n",
    "transfer = True\n",
    "\n",
    "SAE_PATH = \"/workspace/huggingface/hub/models--mech-interp--pythia-160m-deduped-rs-post/snapshots/49befceb8d1f7be1d4b3c6bef477c4e899def430\"\n",
    "\n",
    "#FWD_SAE_PATH = os.path.join(SAE_PATH, 'forward', f\"L{sae_idx}\", ckpt_step)\n",
    "#BWD_SAE_PATH = os.path.join(SAE_PATH, 'backward', f\"L{sae_idx}\", ckpt_step)\n",
    "#BASE_SAE_PATH = os.path.join(SAE_PATH, f\"L{sae_idx}\")\n",
    "\n",
    "#fwd_sae = SAE.load_from_pretrained(FWD_SAE_PATH).to(device)\n",
    "#bwd_sae = SAE.load_from_pretrained(BWD_SAE_PATH).to(device)\n",
    "#base_sae = SAE.load_from_pretrained(BASE_SAE_PATH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store = ActivationsStore.from_config(model, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flatten(nested_list):\n",
    "    return [x for y in nested_list for x in y]\n",
    "\n",
    "# A very handy function Neel wrote to get context around a feature activation\n",
    "def make_token_df(tokens, len_prefix=5, len_suffix=3, model = model):\n",
    "    str_tokens = [model.to_str_tokens(t) for t in tokens]\n",
    "    unique_token = [[f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens]\n",
    "    \n",
    "    context = []\n",
    "    prompt = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for b in range(tokens.shape[0]):\n",
    "        for p in range(tokens.shape[1]):\n",
    "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
    "            if p==tokens.shape[1]-1:\n",
    "                suffix = \"\"\n",
    "            else:\n",
    "                suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
    "            prompt.append(b)\n",
    "            pos.append(p)\n",
    "            label.append(f\"{b}/{p}\")\n",
    "    # print(len(batch), len(pos), len(context), len(label))\n",
    "    return pd.DataFrame(dict(\n",
    "        str_tokens=list_flatten(str_tokens),\n",
    "        unique_token=list_flatten(unique_token),\n",
    "        context=context,\n",
    "        prompt=prompt,\n",
    "        pos=pos,\n",
    "        label=label,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding max activating examples is a bit harder. To do this we need to calculate feature activations for a large number of tokens\n",
    "feature_list = torch.randint(0, sae.cfg.d_sae, (32,))\n",
    "examples_found = 0\n",
    "all_fired_tokens = []\n",
    "all_feature_acts = []\n",
    "all_reconstructions = []\n",
    "all_token_dfs = []\n",
    "\n",
    "total_batches = 32\n",
    "batch_size_prompts = activation_store.store_batch_size_prompts\n",
    "batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
    "pbar = tqdm(range(total_batches))\n",
    "for i in pbar:\n",
    "    tokens = activation_store.get_batch_tokens()\n",
    "    tokens_df = make_token_df(tokens)\n",
    "    tokens_df[\"batch\"] = i\n",
    "    \n",
    "    flat_tokens = tokens.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens, stop_at_layer = sae.cfg.hook_layer + 1, names_filter = [sae.cfg.hook_name])\n",
    "        sae_in = cache[sae.cfg.hook_name]\n",
    "        feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "        feature_acts = feature_acts.flatten(0,1)\n",
    "        fired_mask = (feature_acts[:, feature_list]).sum(dim=-1) > 0\n",
    "        fired_tokens = model.to_str_tokens(flat_tokens[fired_mask])\n",
    "        reconstruction = feature_acts[fired_mask][:, feature_list] @ sae.W_dec[feature_list]\n",
    "\n",
    "    token_df = tokens_df.iloc[fired_mask.cpu().nonzero().flatten().numpy()]\n",
    "    all_token_dfs.append(token_df)\n",
    "    all_feature_acts.append(feature_acts[fired_mask][:, feature_list])\n",
    "    all_fired_tokens.append(fired_tokens)\n",
    "    all_reconstructions.append(reconstruction)\n",
    "    \n",
    "    examples_found += len(fired_tokens)\n",
    "    # print(f\"Examples found: {examples_found}\")\n",
    "    # update description\n",
    "    pbar.set_description(f\"Examples found: {examples_found}\")\n",
    "    del cache\n",
    "    \n",
    "# flatten the list of lists\n",
    "all_token_dfs = pd.concat(all_token_dfs).reset_index(drop=True)\n",
    "all_fired_tokens = list_flatten(all_fired_tokens)\n",
    "all_reconstructions = torch.cat(all_reconstructions)\n",
    "all_feature_acts = torch.cat(all_feature_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a filtering here on feature list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_df = pd.DataFrame(all_feature_acts.detach().cpu().numpy(), columns = [f\"feature_{i}\" for i in feature_list])\n",
    "feature_acts_df.shape\n",
    "feature_idx = 1\n",
    "# get non-zero activations\n",
    "\n",
    "all_positive_acts = all_feature_acts[all_feature_acts[:, feature_idx] > 0][:, feature_idx].detach()\n",
    "prop_positive_activations = 100*len(all_positive_acts) / (total_batches*batch_size_tokens)\n",
    "\n",
    "px.histogram(\n",
    "    all_positive_acts.cpu(),\n",
    "    nbins=50,\n",
    "    title=f\"Histogram of positive activations of F{feature_list[feature_idx]} - {prop_positive_activations:.3f}% of activations were positive\",\n",
    "    labels={\"value\": \"Activation\"},\n",
    "    width=800,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_activations = feature_acts_df.sort_values(f\"feature_{feature_list[feature_idx]}\", ascending=False).head(10)\n",
    "all_token_dfs.iloc[top_10_activations.index].join(feature_acts_df[f\"feature_{feature_list[feature_idx]}\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the decoder weights {sae.W_dec.shape})\")\n",
    "print(f\"Shape of the model unembed {model.W_U.shape}\")\n",
    "projection_matrix = sae.W_dec @ model.W_U\n",
    "print(f\"Shape of the projection matrix {projection_matrix.shape}\")\n",
    "\n",
    "# then we take the top_k tokens per feature and decode them\n",
    "top_k = 10\n",
    "# let's do this for 100 random features\n",
    "_, top_k_tokens = torch.topk(projection_matrix[feature_list], top_k, dim=1)\n",
    "\n",
    "\n",
    "feature_df = pd.DataFrame(top_k_tokens.cpu().numpy(), index = [f\"feature_{i}\" for i in feature_list]).T\n",
    "feature_df.index = [f\"token_{i}\" for i in range(top_k)]\n",
    "top_logits_df = feature_df.map(lambda x: model.tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_logits_df[f\"feature_{feature_list[feature_idx]}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "\n",
    "all_tokens = torch.cat([activation_store.get_batch_tokens() for _ in range(64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_step = \"500M\"\n",
    "\n",
    "features = torch.randint(0, model.cfg.d_model*8, (512,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in tqdm(range(12)):\n",
    "    FWD_SAE_PATH = os.path.join(SAE_PATH, 'forward', f\"L{l}\", ckpt_step)\n",
    "    BWD_SAE_PATH = os.path.join(SAE_PATH, 'backward', f\"L{l}\", ckpt_step)\n",
    "    BASE_SAE_PATH = os.path.join(SAE_PATH, f\"L{l}\")\n",
    "\n",
    "    print(f\"Loading SAEs for layer {l} at {ckpt_step}\")\n",
    "\n",
    "    sae_vis_config = SaeVisConfig(\n",
    "        hook_point = f\"blocks.{l}.hook_resid_post\",\n",
    "        features = features,\n",
    "        verbose = False,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        fwd_sae = SAE.load_from_pretrained(FWD_SAE_PATH).to(device)\n",
    "        fwd_vis_data = SaeVisData.create(\n",
    "            encoder = fwd_sae,\n",
    "            model = model,\n",
    "            tokens = all_tokens,\n",
    "            cfg = sae_vis_config,\n",
    "        )\n",
    "        fwd_vis_data.save_json(f\"vis/l{l}_fwd.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load forward SAE for layer {l} - {e}\")\n",
    "\n",
    "    try:\n",
    "        bwd_sae = SAE.load_from_pretrained(BWD_SAE_PATH).to(device)\n",
    "        bwd_vis_data = SaeVisData.create(\n",
    "            encoder = bwd_sae,\n",
    "            model = model,\n",
    "            tokens = all_tokens,\n",
    "            cfg = sae_vis_config,\n",
    "        )\n",
    "        bwd_vis_data.save_json(f\"vis/l{l}_bwd.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load backward SAE for layer {l} - {e}\")\n",
    "\n",
    "    try:\n",
    "        base_sae = SAE.load_from_pretrained(BASE_SAE_PATH).to(device)\n",
    "        base_vis_data = SaeVisData.create(\n",
    "            encoder = base_sae,\n",
    "            model = model,\n",
    "            tokens = all_tokens,\n",
    "            cfg = sae_vis_config,\n",
    "        )\n",
    "        base_vis_data.save_json(f\"vis/l{l}_base.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load base SAE for layer {l} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_shared_items(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    shared_items = set1.intersection(set2)\n",
    "    return len(shared_items)\n",
    "\n",
    "for l in tqdm(range(1, 11)):\n",
    "    fwd_data = json.load(open(f\"vis/l{l+1}_fwd.json\", 'r'))['feature_data_dict']\n",
    "    bwd_data = json.load(open(f\"vis/l{l-1}_bwd.json\", 'r'))['feature_data_dict']\n",
    "    base_data = json.load(open(f\"vis/l{l}_base.json\", 'r'))['feature_data_dict']\n",
    "\n",
    "    fwd_scores = {\n",
    "        'feature': [],\n",
    "        'top_logits': [],\n",
    "        'bottom_logits': [],\n",
    "        'max_toks': [],\n",
    "    }\n",
    "\n",
    "    bwd_scores = {\n",
    "        'feature': [],\n",
    "        'top_logits': [],\n",
    "        'bottom_logits': [],\n",
    "        'max_toks': [],\n",
    "    }\n",
    "\n",
    "    base_scores = {\n",
    "        'feature': [],\n",
    "        'top_logits': [],\n",
    "        'bottom_logits': [],\n",
    "        'max_toks': [],\n",
    "    }\n",
    "\n",
    "    for data, scores in tqdm(zip([fwd_data, bwd_data, base_data], [fwd_scores, bwd_scores, base_scores])):\n",
    "        for f, f_data in data.items():\n",
    "            logit_data = f_data['logits_table_data']\n",
    "            top_logits = logit_data['top_token_ids']\n",
    "            bottom_logits = logit_data['bottom_token_ids']\n",
    "\n",
    "            token_data = f_data['sequence_data']['seq_group_data'][0]['seq_data']\n",
    "            max_toks = []\n",
    "\n",
    "            for seq in token_data[:10]:\n",
    "                tok_ids, f_acts, loss_contrib = seq['token_ids'], seq['feat_acts'], seq['loss_contribution']\n",
    "                assert len(tok_ids) == len(f_acts) == len(loss_contrib)\n",
    "                max_tok = tok_ids[np.argmax(np.array(f_acts) + np.array(loss_contrib))]\n",
    "                max_toks.append(max_tok)\n",
    "\n",
    "            scores['feature'].append(f)\n",
    "            scores['top_logits'].append(top_logits)\n",
    "            scores['bottom_logits'].append(bottom_logits)\n",
    "            scores['max_toks'].append(max_toks)\n",
    "\n",
    "    fwd_df = pd.DataFrame(fwd_scores)\n",
    "    bwd_df = pd.DataFrame(bwd_scores)\n",
    "    base_df = pd.DataFrame(base_scores)\n",
    "\n",
    "    fwd_df = fwd_df.add_suffix('_fwd')\n",
    "    bwd_df = bwd_df.add_suffix('_bwd')\n",
    "    base_df = base_df.add_suffix('_base')\n",
    "\n",
    "    total_df = pd.concat([fwd_df, bwd_df, base_df], axis=1)\n",
    "\n",
    "    total_df['shared_fwd_top_logits'] = total_df.apply(lambda x: count_shared_items(x['top_logits_fwd'], x['top_logits_base']), axis=1)\n",
    "    total_df['shared_fwd_bottom_logits'] = total_df.apply(lambda x: count_shared_items(x['bottom_logits_fwd'], x['bottom_logits_base']), axis=1)\n",
    "    total_df['shared_fwd_max_toks'] = total_df.apply(lambda x: count_shared_items(x['max_toks_fwd'], x['max_toks_base']), axis=1)\n",
    "\n",
    "    total_df['shared_bwd_top_logits'] = total_df.apply(lambda x: count_shared_items(x['top_logits_bwd'], x['top_logits_base']), axis=1)\n",
    "    total_df['shared_bwd_bottom_logits'] = total_df.apply(lambda x: count_shared_items(x['bottom_logits_bwd'], x['bottom_logits_base']), axis=1)\n",
    "    total_df['shared_bwd_max_toks'] = total_df.apply(lambda x: count_shared_items(x['max_toks_bwd'], x['max_toks_base']), axis=1)\n",
    "\n",
    "    result_df = total_df[['feature_base', 'shared_fwd_top_logits', 'shared_fwd_bottom_logits', 'shared_fwd_max_toks', 'shared_bwd_top_logits', 'shared_bwd_bottom_logits', 'shared_bwd_max_toks']]\n",
    "    result_df.to_csv(f\"vis/l{l}_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4\n",
    "\n",
    "all_results_df = []\n",
    "\n",
    "for l in range(1, 11):\n",
    "    result_df = pd.read_csv(f\"vis/l{l}_result.csv\")\n",
    "    result_df.iloc[:, 1:] = result_df.iloc[:, 1:].map(lambda x: 1 if x >= threshold else 0)\n",
    "    result_df['layer'] = l\n",
    "    \n",
    "    all_results_df.append(result_df)\n",
    "\n",
    "all_results_df = pd.concat(all_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_base</th>\n",
       "      <th>shared_fwd_top_logits</th>\n",
       "      <th>shared_fwd_bottom_logits</th>\n",
       "      <th>shared_fwd_max_toks</th>\n",
       "      <th>shared_bwd_top_logits</th>\n",
       "      <th>shared_bwd_bottom_logits</th>\n",
       "      <th>shared_bwd_max_toks</th>\n",
       "      <th>layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3174</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5555</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1346</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1816</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4880 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_base  shared_fwd_top_logits  shared_fwd_bottom_logits  \\\n",
       "0            3174                      1                         0   \n",
       "1            5555                      0                         0   \n",
       "2            2908                      0                         0   \n",
       "3            1294                      0                         0   \n",
       "4            5226                      0                         0   \n",
       "..            ...                    ...                       ...   \n",
       "483          1346                      1                         1   \n",
       "484           127                      1                         0   \n",
       "485            17                      1                         0   \n",
       "486          1816                      1                         0   \n",
       "487          2025                      0                         0   \n",
       "\n",
       "     shared_fwd_max_toks  shared_bwd_top_logits  shared_bwd_bottom_logits  \\\n",
       "0                      0                      0                         0   \n",
       "1                      0                      1                         1   \n",
       "2                      0                      0                         0   \n",
       "3                      0                      0                         0   \n",
       "4                      0                      0                         0   \n",
       "..                   ...                    ...                       ...   \n",
       "483                    0                      1                         1   \n",
       "484                    0                      0                         0   \n",
       "485                    0                      0                         0   \n",
       "486                    0                      1                         0   \n",
       "487                    0                      0                         0   \n",
       "\n",
       "     shared_bwd_max_toks  layer  \n",
       "0                      0      1  \n",
       "1                      0      1  \n",
       "2                      0      1  \n",
       "3                      0      1  \n",
       "4                      0      1  \n",
       "..                   ...    ...  \n",
       "483                    1     10  \n",
       "484                    0     10  \n",
       "485                    0     10  \n",
       "486                    0     10  \n",
       "487                    0     10  \n",
       "\n",
       "[4880 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df['bwd_feature_transfer'] = (all_results_df['shared_bwd_top_logits'] + all_results_df['shared_bwd_bottom_logits'] + all_results_df['shared_bwd_max_toks']) > 1\n",
    "all_results_df['fwd_feature_transfer'] = (all_results_df['shared_fwd_top_logits'] + all_results_df['shared_fwd_bottom_logits'] + all_results_df['shared_fwd_max_toks']) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Transferability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th>Transfer</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>bwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_feature_transfer</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Transferability\n",
       "layer Transfer                             \n",
       "1     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "2     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "3     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "4     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "5     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "6     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "7     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "8     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "9     bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0\n",
       "10    bwd_feature_transfer              0.0\n",
       "      fwd_feature_transfer              0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_df.groupby(['layer', 'Transfer']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x540 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = all_results_df[['layer', 'fwd_feature_transfer', 'bwd_feature_transfer']].melt(id_vars='layer', var_name='Transfer', value_name='Transferability')\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.5), dpi=120)\n",
    "sns.set_theme()\n",
    "ax = sns.barplot(\n",
    "    data=plot_df.groupby(['layer', 'Transfer']).mean(),\n",
    "    x='layer',\n",
    "    y='Transferability',\n",
    "    hue='Transfer',\n",
    "    palette=palette_duo,\n",
    ")\n",
    "\n",
    "ax.title.set_fontsize(13)\n",
    "ax.xaxis.label.set_fontsize(10)\n",
    "ax.yaxis.label.set_fontsize(10)\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax.xaxis.labelpad = 10\n",
    "ax.yaxis.labelpad = 10\n",
    "\n",
    "# Set title and move legend out of the plot\n",
    "plt.title(\"Feature Transferability\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.legend()\n",
    "sns.move_legend(ax, \"lower center\", bbox_to_anchor=(.5, -0.32), ncol=3, title=None, frameon=False)\n",
    "plt.savefig(\"img/f_transfer.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple, Callable\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "\n",
    "class SaeReconstructionCache(NamedTuple):\n",
    "    sae_in: torch.Tensor\n",
    "    feature_acts: torch.Tensor\n",
    "    sae_out: torch.Tensor\n",
    "    sae_error: torch.Tensor\n",
    "\n",
    "\n",
    "def track_grad(tensor: torch.Tensor) -> None:\n",
    "    \"\"\"wrapper around requires_grad and retain_grad\"\"\"\n",
    "    tensor.requires_grad_(True)\n",
    "    tensor.retain_grad()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ApplySaesAndRunOutput:\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Helper to zero grad all tensors in this object.\"\"\"\n",
    "        self.model_output.grad = None\n",
    "        for act in self.model_activations.values():\n",
    "            act.grad = None\n",
    "        for cache in self.sae_activations.values():\n",
    "            cache.sae_in.grad = None\n",
    "            cache.feature_acts.grad = None\n",
    "            cache.sae_out.grad = None\n",
    "            cache.sae_error.grad = None\n",
    "\n",
    "\n",
    "def apply_saes_and_run(\n",
    "    model: HookedTransformer,\n",
    "    saes: dict[str, SAE],\n",
    "    input: Any,\n",
    "    include_error_term: bool = True,\n",
    "    track_model_hooks: list[str] | None = None,\n",
    "    return_type: Literal[\"logits\", \"loss\"] = \"logits\",\n",
    "    track_grads: bool = False,\n",
    ") -> ApplySaesAndRunOutput:\n",
    "    \"\"\"\n",
    "    Apply the SAEs to the model at the specific hook points, and run the model.\n",
    "    By default, this will include a SAE error term which guarantees that the SAE\n",
    "    will not affect model output. This function is designed to work correctly with\n",
    "    backprop as well, so it can be used for gradient-based feature attribution.\n",
    "\n",
    "    Args:\n",
    "        model: the model to run\n",
    "        saes: the SAEs to apply\n",
    "        input: the input to the model\n",
    "        include_error_term: whether to include the SAE error term to ensure the SAE doesn't affect model output. Default True\n",
    "        track_model_hooks: a list of hook points to record the activations and gradients. Default None\n",
    "        return_type: this is passed to the model.run_with_hooks function. Default \"logits\"\n",
    "        track_grads: whether to track gradients. Default False\n",
    "    \"\"\"\n",
    "\n",
    "    fwd_hooks = []\n",
    "    bwd_hooks = []\n",
    "\n",
    "    sae_activations: dict[str, SaeReconstructionCache] = {}\n",
    "    model_activations: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # this hook just track the SAE input, output, features, and error. If `track_grads=True`, it also ensures\n",
    "    # that requires_grad is set to True and retain_grad is called for intermediate values.\n",
    "    def reconstruction_hook(sae_in: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        sae = saes[hook_point]\n",
    "        feature_acts = sae.encode(sae_in)\n",
    "        sae_out = sae.decode(feature_acts)\n",
    "        sae_error = (sae_in - sae_out).detach().clone()\n",
    "        if track_grads:\n",
    "            track_grad(sae_error)\n",
    "            track_grad(sae_out)\n",
    "            track_grad(feature_acts)\n",
    "            track_grad(sae_in)\n",
    "        sae_activations[hook_point] = SaeReconstructionCache(\n",
    "            sae_in=sae_in,\n",
    "            feature_acts=feature_acts,\n",
    "            sae_out=sae_out,\n",
    "            sae_error=sae_error,\n",
    "        )\n",
    "\n",
    "        if include_error_term:\n",
    "            return sae_out + sae_error\n",
    "        return sae_out\n",
    "\n",
    "    def sae_bwd_hook(output_grads: torch.Tensor, hook: HookPoint):  # noqa: ARG001\n",
    "        # this just passes the output grads to the input, so the SAE gets the same grads despite the error term hackery\n",
    "        return (output_grads,)\n",
    "\n",
    "    # this hook just records model activations, and ensures that intermediate activations have gradient tracking turned on if needed\n",
    "    def tracking_hook(hook_input: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        model_activations[hook_point] = hook_input\n",
    "        if track_grads:\n",
    "            track_grad(hook_input)\n",
    "        return hook_input\n",
    "\n",
    "    for hook_point in saes.keys():\n",
    "        fwd_hooks.append(\n",
    "            (hook_point, partial(reconstruction_hook, hook_point=hook_point))\n",
    "        )\n",
    "        bwd_hooks.append((hook_point, sae_bwd_hook))\n",
    "    for hook_point in track_model_hooks or []:\n",
    "        fwd_hooks.append((hook_point, partial(tracking_hook, hook_point=hook_point)))\n",
    "\n",
    "    # now, just run the model while applying the hooks\n",
    "    with model.hooks(fwd_hooks=fwd_hooks, bwd_hooks=bwd_hooks):\n",
    "        model_output = model(input, return_type=return_type)\n",
    "\n",
    "    return ApplySaesAndRunOutput(\n",
    "        model_output=model_output,\n",
    "        model_activations=model_activations,\n",
    "        sae_activations=sae_activations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import HookedSAETransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "@dataclass\n",
    "class AttributionGrads:\n",
    "    metric: torch.Tensor\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Attribution:\n",
    "    model_attributions: dict[str, torch.Tensor]\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    model_grads: dict[str, torch.Tensor]\n",
    "    sae_feature_attributions: dict[str, torch.Tensor]\n",
    "    sae_feature_activations: dict[str, torch.Tensor]\n",
    "    sae_feature_grads: dict[str, torch.Tensor]\n",
    "    sae_errors_attribution_proportion: dict[str, float]\n",
    "\n",
    "\n",
    "def calculate_attribution_grads(\n",
    "    model: HookedSAETransformer,\n",
    "    prompt: str,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> AttributionGrads:\n",
    "    \"\"\"\n",
    "    Wrapper around apply_saes_and_run that calculates gradients wrt to the metric_fn.\n",
    "    Tracks grads for both SAE feature and model neurons, and returns them in a structured format.\n",
    "    \"\"\"\n",
    "    output = apply_saes_and_run(\n",
    "        model,\n",
    "        saes=include_saes or {},\n",
    "        input=prompt,\n",
    "        return_type=\"logits\" if return_logits else \"loss\",\n",
    "        track_model_hooks=track_hook_points,\n",
    "        include_error_term=include_error_term,\n",
    "        track_grads=True,\n",
    "    )\n",
    "    metric = metric_fn(output.model_output)\n",
    "    output.zero_grad()\n",
    "    metric.backward()\n",
    "    return AttributionGrads(\n",
    "        metric=metric,\n",
    "        model_output=output.model_output,\n",
    "        model_activations=output.model_activations,\n",
    "        sae_activations=output.sae_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_feature_attribution(\n",
    "    model: HookedSAETransformer,\n",
    "    input: Any,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> Attribution:\n",
    "    \"\"\"\n",
    "    Calculate feature attribution for SAE features and model neurons following\n",
    "    the procedure in https://transformer-circuits.pub/2024/march-update/index.html#feature-heads.\n",
    "    This include the SAE error term by default, so inserting the SAE into the calculation is\n",
    "    guaranteed to not affect the model output. This can be disabled by setting `include_error_term=False`.\n",
    "\n",
    "    Args:\n",
    "        model: The model to calculate feature attribution for.\n",
    "        input: The input to the model.\n",
    "        metric_fn: A function that takes the model output and returns a scalar metric.\n",
    "        track_hook_points: A list of model hook points to track activations for, if desired\n",
    "        include_saes: A dictionary of SAEs to include in the calculation. The key is the hook point to apply the SAE to.\n",
    "        return_logits: Whether to return the model logits or loss. This is passed to TLens, so should match whatever the metric_fn expects (probably logits)\n",
    "        include_error_term: Whether to include the SAE error term in the calculation. This is recommended, as it ensures that the SAE will not affecting the model output.\n",
    "    \"\"\"\n",
    "    # first, calculate gradients wrt to the metric_fn.\n",
    "    # these will be multiplied with the activation values to get the attributions\n",
    "    outputs_with_grads = calculate_attribution_grads(\n",
    "        model,\n",
    "        input,\n",
    "        metric_fn,\n",
    "        track_hook_points,\n",
    "        include_saes=include_saes,\n",
    "        return_logits=return_logits,\n",
    "        include_error_term=include_error_term,\n",
    "    )\n",
    "    model_attributions = {}\n",
    "    model_activations = {}\n",
    "    model_grads = {}\n",
    "    sae_feature_attributions = {}\n",
    "    sae_feature_activations = {}\n",
    "    sae_feature_grads = {}\n",
    "    sae_error_proportions = {}\n",
    "    # this code is long, but all it's doing is multiplying the grads by the activations\n",
    "    # and recording grads, acts, and attributions in dictionaries to return to the user\n",
    "    with torch.no_grad():\n",
    "        for name, act in outputs_with_grads.model_activations.items():\n",
    "            assert act.grad is not None\n",
    "            raw_activation = act.detach().clone()\n",
    "            model_attributions[name] = (act.grad * raw_activation).detach().clone()\n",
    "            model_activations[name] = raw_activation\n",
    "            model_grads[name] = act.grad.detach().clone()\n",
    "        for name, act in outputs_with_grads.sae_activations.items():\n",
    "            assert act.feature_acts.grad is not None\n",
    "            assert act.sae_out.grad is not None\n",
    "            raw_activation = act.feature_acts.detach().clone()\n",
    "            sae_feature_attributions[name] = (\n",
    "                (act.feature_acts.grad * raw_activation).detach().clone()\n",
    "            )\n",
    "            sae_feature_activations[name] = raw_activation\n",
    "            sae_feature_grads[name] = act.feature_acts.grad.detach().clone()\n",
    "            if include_error_term:\n",
    "                assert act.sae_error.grad is not None\n",
    "                error_grad_norm = act.sae_error.grad.norm().item()\n",
    "            else:\n",
    "                error_grad_norm = 0\n",
    "            sae_out_norm = act.sae_out.grad.norm().item()\n",
    "            sae_error_proportions[name] = error_grad_norm / (\n",
    "                sae_out_norm + error_grad_norm + EPS\n",
    "            )\n",
    "        return Attribution(\n",
    "            model_attributions=model_attributions,\n",
    "            model_activations=model_activations,\n",
    "            model_grads=model_grads,\n",
    "            sae_feature_attributions=sae_feature_attributions,\n",
    "            sae_feature_activations=sae_feature_activations,\n",
    "            sae_feature_grads=sae_feature_grads,\n",
    "            sae_errors_attribution_proportion=sae_error_proportions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_df = pd.read_csv(\"eval/test_attr.csv\")\n",
    "\n",
    "def check_single_token(x):\n",
    "    try:\n",
    "        model.to_single_token(\" \" + x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "attr_df['Correct Single Token'] = attr_df['Correct Answer'].apply(check_single_token)\n",
    "attr_df['Wrong Single Token'] = attr_df['Wrong Answer'].apply(check_single_token)\n",
    "\n",
    "attr_df = attr_df[(attr_df['Correct Single Token']) & (attr_df['Wrong Single Token'])].iloc[:, :-2].drop_duplicates()\n",
    "attr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(idx):\n",
    "    row = attr_df.iloc[idx]\n",
    "    prompt = row['Sentence']\n",
    "    \n",
    "    pos_token = model.tokenizer.encode(\" \" + row['Correct Answer'], add_special_tokens=False)\n",
    "    neg_token = model.tokenizer.encode(\" \" + row['Wrong Answer'], add_special_tokens=False)\n",
    "\n",
    "    return prompt, pos_token, neg_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_feature_to_long_df(sparse_tensor: torch.Tensor) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a sparse tensor to a long format pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(sparse_tensor.detach().cpu().numpy())\n",
    "    df_long = df.melt(ignore_index=False, var_name='column', value_name='value')\n",
    "    df_long.columns = [\"feature\", \"attribution\"]\n",
    "    df_long_nonzero = df_long[df_long['attribution'] != 0]\n",
    "    df_long_nonzero = df_long_nonzero.reset_index().rename(columns={'index': 'position'})\n",
    "    return df_long_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_step = \"100M\"\n",
    "\n",
    "base_fa_scores = []\n",
    "fwd_fa_scores = []\n",
    "bwd_fa_scores = []\n",
    "\n",
    "for l in tqdm(range(1,11)):\n",
    "    \n",
    "    FWD_SAE_PATH = os.path.join(SAE_PATH, 'forward', f\"L{l}\", ckpt_step)\n",
    "    BWD_SAE_PATH = os.path.join(SAE_PATH, 'backward', f\"L{l}\", ckpt_step)\n",
    "    BASE_SAE_PATH = os.path.join(SAE_PATH, f\"L{l}\")\n",
    "\n",
    "    fwd_sae = SAE.load_from_pretrained(FWD_SAE_PATH).to(device)\n",
    "    bwd_sae = SAE.load_from_pretrained(BWD_SAE_PATH).to(device)\n",
    "    base_sae = SAE.load_from_pretrained(BASE_SAE_PATH).to(device)\n",
    "    \n",
    "    for i in range(attr_df.shape[0]):\n",
    "        prompt, pos_token, neg_token = get_prompt(i)\n",
    "        \n",
    "        def metric_fn(logits: torch.tensor, pos_token: torch.tensor =pos_token, neg_token: torch.Tensor=neg_token) -> torch.Tensor:\n",
    "            return logits[0,-1,pos_token] - logits[0,-1,neg_token]\n",
    "\n",
    "        base_fa = calculate_feature_attribution(\n",
    "            input = prompt,\n",
    "            model = model,\n",
    "            metric_fn = metric_fn,\n",
    "            include_saes={base_sae.cfg.hook_name: base_sae},\n",
    "            include_error_term=True,\n",
    "            return_logits=True,\n",
    "        )\n",
    "\n",
    "        #fwd_fa = calculate_feature_attribution(\n",
    "        #    input = prompt,\n",
    "        #    model = model,\n",
    "        #    metric_fn = metric_fn,\n",
    "        #    include_saes={fwd_sae.cfg.hook_name: fwd_sae},\n",
    "        #    include_error_term=True,\n",
    "        #    return_logits=True,\n",
    "        #)\n",
    "\n",
    "        #bwd_fa = calculate_feature_attribution(\n",
    "        #    input = prompt,\n",
    "        #    model = model,\n",
    "        #    metric_fn = metric_fn,\n",
    "        #    include_saes={bwd_sae.cfg.hook_name: bwd_sae},\n",
    "        #    include_error_term=True,\n",
    "        #    return_logits=True,\n",
    "        #)\n",
    "\n",
    "        base_fa_df = convert_sparse_feature_to_long_df(base_fa.sae_feature_attributions[base_sae.cfg.hook_name][0])\n",
    "        #fwd_fa_df = convert_sparse_feature_to_long_df(fwd_fa.sae_feature_attributions[fwd_sae.cfg.hook_name][0])\n",
    "        #bwd_fa_df = convert_sparse_feature_to_long_df(bwd_fa.sae_feature_attributions[bwd_sae.cfg.hook_name][0])\n",
    "\n",
    "        base_fa_scores.append(base_fa_df['attribution'].max())\n",
    "        #fwd_fa_scores.append(fwd_fa_df['attribution'].max())\n",
    "        #bwd_fa_scores.append(bwd_fa_df['attribution'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "scores_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Layer\": np.arange(1,11)[:, None].repeat(len(attr_df)),\n",
    "        \"No transfer\": base_fa_scores,\n",
    "        #\"Forward\": fwd_fa_scores,\n",
    "        #\"Backward\": bwd_fa_scores,\n",
    "    }\n",
    ")\n",
    "\n",
    "scores_df = scores_df.melt(id_vars='Layer', var_name='Transfer', value_name='Max Attribution Score')\n",
    "#scores_df.to_csv(f\"eval/dla_scores_base.csv\", index=False)\n",
    "avg_scores_df = scores_df.groupby(['Layer', 'Transfer']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dla_scores_base = pd.read_csv(\"eval/dla_scores_base.csv\")\n",
    "dla_scores_100 = pd.read_csv(\"eval/dla_scores_100M.csv\")\n",
    "dla_scores_200 = pd.read_csv(\"eval/dla_scores_200M.csv\")\n",
    "dla_scores_300 = pd.read_csv(\"eval/dla_scores_300M.csv\")\n",
    "dla_scores_400 = pd.read_csv(\"eval/dla_scores_400M.csv\")\n",
    "dla_scores_500 = pd.read_csv(\"eval/dla_scores_500M.csv\")\n",
    "\n",
    "dla_scores_100['Checkpoint'] = '100M'\n",
    "dla_scores_200['Checkpoint'] = '200M'\n",
    "dla_scores_300['Checkpoint'] = '300M'\n",
    "dla_scores_400['Checkpoint'] = '400M'\n",
    "dla_scores_500['Checkpoint'] = '500M'\n",
    "\n",
    "dla_scores = pd.concat([dla_scores_100, dla_scores_200, dla_scores_300, dla_scores_400, dla_scores_500])\n",
    "dla_scores = dla_scores[dla_scores['Transfer'] != 'No transfer']\n",
    "avg_scores_df = dla_scores.groupby(['Transfer', 'Checkpoint']).mean().reset_index()\n",
    "avg_base_scores = dla_scores_base.groupby(['Transfer']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=120)\n",
    "sns.set_theme()\n",
    "ax0 = sns.lineplot(\n",
    "    avg_scores_df,\n",
    "    x=\"Checkpoint\",\n",
    "    y=\"Max Attribution Score\",\n",
    "    hue=\"Transfer\",\n",
    "    marker='o',\n",
    "    markersize=6,\n",
    "    palette=palette_duo,\n",
    "    hue_order=['Forward', 'Backward']\n",
    ")\n",
    "\n",
    "plt.hlines(avg_base_scores['Max Attribution Score'], 0, 4, linestyles='dashed', label='No transfer', color='grey')\n",
    "\n",
    "\n",
    "ax0.title.set_fontsize(13)\n",
    "ax0.xaxis.label.set_fontsize(10)\n",
    "ax0.yaxis.label.set_fontsize(10)\n",
    "ax0.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax0.xaxis.labelpad = 10\n",
    "ax0.yaxis.labelpad = 10\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title(\"Direct Logit Attribution Scores\")\n",
    "plt.ylabel(\"Avg. Attribution Score\")\n",
    "# add baseline to legend\n",
    "plt.legend()\n",
    "sns.move_legend(ax0, \"lower center\", bbox_to_anchor=(.5, -0.35), ncol=3, title=None, frameon=False)\n",
    "plt.savefig(\"img/dla_scores.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_dla_scores = dla_scores[dla_scores['Transfer'] == 'Forward'].drop(columns=['Transfer'])\n",
    "bwd_dla_scores = dla_scores[dla_scores['Transfer'] == 'Backward'].drop(columns=['Transfer'])\n",
    "\n",
    "avg_fwd_scores = fwd_dla_scores.groupby(['Checkpoint', 'Layer']).mean().reset_index()\n",
    "avg_bwd_scores = bwd_dla_scores.groupby(['Checkpoint', 'Layer']).mean().reset_index()\n",
    "avg_base_scores = dla_scores_base.groupby(['Transfer', 'Layer']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10.5, 4.5), dpi=120)\n",
    "sns.set_theme()\n",
    "ax0 = sns.lineplot(\n",
    "    avg_fwd_scores,\n",
    "    x=\"Checkpoint\",\n",
    "    y=\"Max Attribution Score\",\n",
    "    hue=\"Layer\",\n",
    "    ax=ax[0],\n",
    "    legend=False,\n",
    "    marker='o',\n",
    "    markersize=6,\n",
    "    palette=palette,\n",
    ")\n",
    "\n",
    "ax1 = sns.lineplot(\n",
    "    avg_bwd_scores,\n",
    "    x=\"Checkpoint\",\n",
    "    y=\"Max Attribution Score\",\n",
    "    hue=\"Layer\",\n",
    "    ax=ax[1],\n",
    "    legend='full',\n",
    "    marker='o',\n",
    "    markersize=6,\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# Set plot title and labels\n",
    "ax[0].set_title(\"Forward Feature Attribution Scores\")\n",
    "ax[0].set_ylabel(\"Attribution Score\")\n",
    "ax[0].set_xlabel(\"Checkpoint\")\n",
    "\n",
    "ax[1].set_title(\"Backward Feature Attribution Scores\")\n",
    "ax[1].set_ylabel(\"Attribution Score\")\n",
    "ax[1].set_xlabel(\"Checkpoint\")\n",
    "\n",
    "for a in ax:\n",
    "    a.title.set_fontsize(13)\n",
    "    a.xaxis.label.set_fontsize(10)\n",
    "    a.yaxis.label.set_fontsize(10)\n",
    "    a.tick_params(axis='both', which='major', labelsize=10)\n",
    "    a.xaxis.labelpad = 10\n",
    "    a.yaxis.labelpad = 10\n",
    "\n",
    "# Adjust legend placement\n",
    "ax[1].legend(title=\"Layer\", bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "# Apply tight layout to adjust all elements\n",
    "plt.tight_layout()\n",
    "sns.move_legend(ax1, \"lower center\", bbox_to_anchor=(-0.13, -0.35), ncol=5, title=None, frameon=False)\n",
    "plt.savefig(\"img/dla_indv_scores.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_step = \"500M\"\n",
    "\n",
    "base_fwd_similarities = []\n",
    "base_bwd_similarities = []\n",
    "\n",
    "for i in tqdm(range(1, 11)):\n",
    "    BASE_SAE_PATH = os.path.join(SAE_PATH, f\"L{i}\", ckpt_step)\n",
    "    base_sae = SAE.load_from_pretrained(BASE_SAE_PATH).to(device)\n",
    "\n",
    "    base_W_dec = base_sae.W_dec\n",
    "\n",
    "    FWD_SAE_PATH = os.path.join(SAE_PATH, 'forward', f\"L{i}\", ckpt_step)\n",
    "    BWD_SAE_PATH = os.path.join(SAE_PATH, 'backward', f\"L{i}\", ckpt_step)\n",
    "    \n",
    "    fwd_sae = SAE.load_from_pretrained(FWD_SAE_PATH).to(device)\n",
    "    bwd_sae = SAE.load_from_pretrained(BWD_SAE_PATH).to(device)\n",
    "    \n",
    "    fwd_W_dec = fwd_sae.W_dec\n",
    "    bwd_W_dec = bwd_sae.W_dec\n",
    "\n",
    "    base_fwd_sim = (fwd_W_dec @ base_W_dec.T).max(-1).values.mean()\n",
    "    base_bwd_sim = (bwd_W_dec @ base_W_dec.T).max(-1).values.mean()\n",
    "\n",
    "    base_fwd_similarities.append(base_fwd_sim.item())\n",
    "    base_bwd_similarities.append(base_bwd_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4), dpi=120)\n",
    "sns.set_theme()\n",
    "ax0 = sns.lineplot(\n",
    "    x=np.arange(1, 11),\n",
    "    y=base_fwd_similarities,\n",
    "    marker='o',\n",
    "    markersize=6,\n",
    "    color=palette[0],\n",
    ")\n",
    "\n",
    "ax1 = sns.lineplot(\n",
    "    x=np.arange(1, 11),\n",
    "    y=base_bwd_similarities,\n",
    "    legend=False,\n",
    "    marker='o',\n",
    "    markersize=6,\n",
    "    color=palette[-1],\n",
    ")\n",
    "\n",
    "ax0.title.set_fontsize(13)\n",
    "ax0.xaxis.label.set_fontsize(10)\n",
    "ax0.yaxis.label.set_fontsize(10)\n",
    "ax0.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax0.xaxis.labelpad = 10\n",
    "ax0.yaxis.labelpad = 10\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title(\"MMCS between Transfer and No transfer SAEs\")\n",
    "plt.ylabel(\"MMCS\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.legend(title=\"Transfer\", labels=[\"Forward\", \"_\", \"Backward\", \"_\"], bbox_to_anchor=(1.03, 1), loc='upper left')\n",
    "plt.ylim(0.59, 0.91)\n",
    "sns.move_legend(ax0, \"lower center\", bbox_to_anchor=(.5, -0.35), ncol=3, title=None, frameon=False)\n",
    "plt.savefig(\"img/mmcs_scores.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_df = pd.read_csv(\"eval/human_interp.csv\").fillna(\"y\").drop(\"Feature\", axis=1)\n",
    "hi_df = hi_df.melt(id_vars='Layer', var_name='Transfer', value_name='Human Interpretability Score')\n",
    "hi_df['Human Interpretability Score'] = hi_df['Human Interpretability Score'].apply(lambda x: 0 if x == \"n\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_mask = (hi_df['Layer'] == 0) & (hi_df['Transfer'] == 'Forward')\n",
    "bwd_mask = (hi_df['Layer'] == 11) & (hi_df['Transfer'] == 'Backward')\n",
    "\n",
    "hi_df.loc[fwd_mask, 'Human Interpretability Score'] = 0\n",
    "hi_df.loc[bwd_mask, 'Human Interpretability Score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7.5, 4.5), dpi=120)\n",
    "sns.set_theme()\n",
    "ax = sns.barplot(\n",
    "    hi_df,\n",
    "    x=\"Layer\",\n",
    "    y=\"Human Interpretability Score\",\n",
    "    hue=\"Transfer\",\n",
    "    orient=\"v\",\n",
    "    hue_order=[\"Forward\", \"Backward\", \"No transfer\"],\n",
    "    palette=palette_duo + ['darkgrey'],\n",
    ")\n",
    "\n",
    "ax.title.set_fontsize(13)\n",
    "ax.xaxis.label.set_fontsize(10)\n",
    "ax.yaxis.label.set_fontsize(10)\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax.xaxis.labelpad = 10\n",
    "ax.yaxis.labelpad = 10\n",
    "\n",
    "# Set title and move legend out of the plot\n",
    "plt.title(\"Human Interpretability Scores\")\n",
    "plt.legend()\n",
    "sns.move_legend(ax, \"lower center\", bbox_to_anchor=(.5, -0.32), ncol=3, title=None, frameon=False)\n",
    "plt.savefig(\"img/hi_scores.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
